---
title: "Modèles linéaires en grande dimension"
output: html_notebook
author: "MAHEO Manon - PENISSON Valentin"
date: "10 janvier 2022"
---

## Packages : 

https://cran.r-project.org/web/packages/dbscan/readme/README.html

```{r}
library(dbscan)
library(ggpubr)
library(factoextra)
library(ggplot2)
library(fpc)
```


## Exercice 1  : Jeux de données  de petite dimension

Sur un jeu de données de dimension 2 ou 3 de votre choix, par exemple un des jeux de données dans **Données**, ou **sismic_data.csv** ou autre :

#### Importation des données

Nous allons travaillé sur le jeu de données **Lsun.lrn**. Ce jeu de données est composé de 400 lignes et 3 colonnes : une colonne x, une colonne y et la colonne des labels.

```{r}
P <-  read.csv(file = "Lsun.lrn", header = FALSE, sep = "\t", skip = 4)[,-1]
true_label <-  as.factor(read.csv(file = "Lsun.cls", header = FALSE, sep = "\t",skip = 1)[,-c(1,3)])
data <-  data.frame(x = P[,1], y = P[,2],true_label = as.factor(true_label))
head(data)
```
```{r}
g <- ggplot(data = data, aes(x=x, y =y)) + geom_point(aes(shape = true_label))
g
```

### Question 1 : Implémenter deux méthodes de partitionnement de votre choix (i.e. méthode de classification non supervisée vue en cours)

#### Méthode 1 : k-means

```{r}
# On choisit k = 3 car il y a trois labels différents
set.seed(123)
res.km <- kmeans(data[,1:2], 3 , nstart = 25)
predict_label.km <- as.character(res.km$cluster)
```

#### Méthode 2 :

```{r}
set.seed(123)
res.dbscan <- dbscan(data[,1:2], eps = 0.5, MinPts = )
predict_label.dbscan <- as.factor(res.dbscan$cluster)
```


### Question 2 : Bien présenter les résultats obtenus, les comparer visuellement aux vraies étiquettes lorsqu'elles sont connues.

#### Méthode 1 : k-means

```{r}
fviz_cluster(res.km, data = data[,1:2],
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```
Ce graphique représente, grâce aux couleurs le vrai label et grâce à la formela valeur prédite. Cette première méthode des k-means avec k = 3 retrouve une partie nos trois groupes de départ. En effet, quelques individus du groupe 1 sont associés au groupe 3 (orange) tout comme des individus du groupe 

#### Méthode 2 : dbscan

```{r}
fviz_cluster(res.dbscan, data = data[,1:2],
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```
La méthode dbscan, quant à elle, classe parfaitement chaque individu dans leur classe respective.

### Question 3 : Pour chacune des méthodes utilisées, expliquer son fonctionnement

#### Méthode 1 : k-means

L'algorithme des k-means est un algorithme de classification non supervisée, c'est à dire qu'à partir simplement de données explicatives 'X', il définit des k groupes. En effet, la classification a pour objectif de consituter des groupes aussi homogènes que possible tout en  étant bien distinctement séparés.

L'algorithme des k-means cherche donc à définir K classes $C_1, C_2, ..., C_K$ tels que $\cup_{k=1}^K C_k = \mathbb{X} $. son objectif est de minimiser l'inertie intra-classes, c'est à dire de trouver les K centres $c_1^*, c_2^*, ..., c_K^*$ et les K classes $C_1^*, C_2^*, ..., C_K^*$ mnimisant : $(c_1,c_2,...,c_K) \mapsto \frac{1}{n} \sum_{i=1}^N min_{k=1,...,K} ||x-\overline{x}_{C_k}||^2$. L'algorithme fonctionne de la façon suivante : 

1 : on chosit un nombre de classes k (ici k = 3). 
2 : on choisit aléatoirement $c_1^*, c_2^*, ..., c_K^*$, nos K centroïdes.
3 : pour chaque point $x_i$, on cherche le centroïde $c_1^*, c_2^*, ..., c_K^*$ le plus proche et on lui associe $x_i$.
4 : pour chaque classe $j = 1,...K$ : on calcule les nouvelles coordonnées du centroïde en faisant la moyenne des points qui lui ont été associés.
5 : on répète les étapes 3 et 4 un grand nombre de fois (par exemple 10 000) ou jusqu'à convergence.


#### Méthode 2 : dbscan

Tout comme l'algorithme des k-means, DBSCAN est un algorithme de classification non supervisée. Sa particularité est qu'il prend en compte la géométrie des données en s'appuyant sur une fonction de distance. En effet, cette méthode s'appuie sur deux paramètres : la distance $\epsilon$ et $MinPts$, le nombre de points minimum devant se trouver dans un rayon $\epsilon$ pour être considérés comme une classe. Cette méthode créee donc, en fonction de ces paramètres, un certain nombre de classes : ce n'est donc pas l'utilisateur qui le définit au préalable. L'idée de cet algorithme est de prendre un point, d'observer ses points voisins dans le rayon $\epsilon$, et de regarder s'il vérifit bien $MinPts$. Dans ce cas, le point choisit fait parti d'un cluster. 

L'algorithme DBSCAN fonctionne de la façon suivante : 

pour chaque point p non visité du jeu de données : 
  marquer comme vu le point p
  chercher tous les points voisins de p (= tous les points autour de p avec un rayon $\epsilon$).
  si p a moins de voisins que $MinPts$ :
          on définit p comme un point aberrant
      sinon :
          on ajoute p à une classe C
          pour chaque point p' des points voisins de p:
            si p' n'a pas été visité:
              marquer p' comme visité
              chercher tous les points voisins de p'
              si p' a plus de voisins que $MinPts$ :
                jointure des voisins de p et de p'
            si p' n'a pas encore de classe C:
              ajouter p' à C
              
## Exercice 2

### Question 1 : Appliquer une méthode de partitionnement de données sur un jeu de données réelles de votre choix, issues par exemple de : *https://www.cs.ucr.edu/~eamonn/time_series_data_2018/*
